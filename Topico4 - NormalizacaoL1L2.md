# L1 e L2 

Na **an√°lise explorat√≥ria de dados (EDA)**, o uso de **normaliza√ß√£o L1 e L2** pode ser √∫til em diversos contextos, especialmente quando lidamos com algoritmos sens√≠veis √† escala dos dados ou quando buscamos compreender a distribui√ß√£o relativa de caracter√≠sticas em um conjunto de dados. A seguir, explico o que s√£o essas normaliza√ß√µes e os **motivos** pelos quais voc√™ poderia us√°-las durante a EDA:

---

### üîß O que s√£o L1 e L2?

- **Normaliza√ß√£o L1 (Manhattan norm):**
  - Transforma cada amostra (linha) de forma que a **soma dos valores absolutos** das suas caracter√≠sticas seja igual a 1.
  - F√≥rmula:  
    $$
    
    x_{i}^{\text{norm}} = \frac{x_i}{\sum_{j=1}^{n} |x_j|}
    
    $$

- **Normaliza√ß√£o L2 (Euclidean norm):**
  - Transforma cada amostra de forma que a **soma dos quadrados** das suas caracter√≠sticas seja igual a 1 (ou seja, norma euclidiana = 1).
  - F√≥rmula:  
   $$
    x_{i}^{\text{norm}} = \frac{x_i}{\sqrt{\sum_{j=1}^{n} x_j^2}}
    $$

---

### üéØ Motivos para usar L1 ou L2 na An√°lise Explorat√≥ria

1. **Comparabilidade entre amostras**  
   Em conjuntos de dados com diferentes escalas ou unidades (e.g., metros, segundos, graus Celsius), normalizar os dados permite compar√°-los mais facilmente.

2. **Redu√ß√£o do vi√©s por magnitude**  
   Quando h√° colunas com valores numericamente maiores, elas podem dominar an√°lises como PCA ou clustering. Normalizar evita esse vi√©s.

3. **Melhora visualiza√ß√µes (como PCA, t-SNE, etc.)**  
   Transforma√ß√µes L1/L2 s√£o √∫teis antes de t√©cnicas de redu√ß√£o de dimensionalidade que se baseiam em dist√¢ncia.

4. **An√°lise de propor√ß√µes (L1)**  
   A normaliza√ß√£o L1 transforma os vetores de caracter√≠sticas em **propor√ß√µes relativas**, o que pode ajudar a interpretar dados como perfis ou distribui√ß√µes (ex: % de gasto por categoria).

5. **Preserva√ß√£o da dire√ß√£o (L2)**  
   A normaliza√ß√£o L2 mant√©m a **dire√ß√£o do vetor** original, √∫til para modelos e an√°lises que se baseiam em √¢ngulos ou dire√ß√µes no espa√ßo vetorial (como em vetores TF-IDF de texto).

6. **Prepara√ß√£o para algoritmos de machine learning**  
   Durante a EDA, √© comum testar algoritmos simples (como KNN ou SVM), e muitos desses s√£o sens√≠veis √† escala ‚Äî por isso, normaliza√ß√µes L1/L2 ajudam na compara√ß√£o justa.

7. **Detec√ß√£o de outliers mais eficiente**  
   Ap√≥s normalizar, outliers podem se destacar mais claramente em gr√°ficos ou an√°lises de dist√¢ncia, pois as dist√¢ncias passam a refletir **diferen√ßas relativas**, e n√£o absolutas.

---

### üß† Quando escolher L1 vs L2?

| Situa√ß√£o | Prefira L1 | Prefira L2 |
|---------|------------|------------|
| Dados esparsos (muitos zeros) | ‚úÖ Sim | ‚ùå N√£o recomendado |
| Propor√ß√µes/import√¢ncia relativa | ‚úÖ Sim | ‚ùå Menos interpret√°vel |
| Preservar dire√ß√£o do vetor | ‚ùå N√£o garante | ‚úÖ Sim |
| Robustez a outliers | ‚úÖ Maior | ‚ùå Menor |
| Aplica√ß√µes de texto (TF-IDF, etc.) | ‚úÖ Comum | ‚úÖ Tamb√©m comum |

---

